---
title: "Edwin Lawson Interview 2 Analysis"
output: html_document
date: "2023-07-26"
name: "Delphine Demaisy"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r install_packages}
library(pdftools)
library(tidyverse)
library(tidytext)
library(dplyr)
library(textclean)
library(stopwords)
library(ggplot2) #for geom_segment location
library(ggraph) #for geom edge link location
library(wordcloud) #for wordcloud
library(RColorBrewer) #for wordcloud
library(topicmodels) #for LDA topic modelling
library(reshape2) #for LDA topic modelling
library(igraph) #for network graph
library(shiny) #for shiny app
```

## Reading in the data
```{r read_data}
lawson2_full_text <- pdftools::pdf_text(pdf = "data/Lawson_Edwin_10201972.pdf")
```

```{r string-data}
# convert text to string
lawson2_full_text <- toString(lawson2_full_text)
lawson2_full_text
# convert text to character lines
lawson2 <- read_lines(lawson2_full_text)

lawson2 <- tibble(lawson2)
lawson2 <- rename(lawson2, text = lawson2)
```

## Cleaning table
```{r data_cleaning}
lawson2 <- lawson2 %>% 
  mutate(initials = str_extract(text, pattern = "[A-Z]{1,3}\\:")) %>%
  fill(initials, .direction = "down")
#remove the metadata and only look at interview
lawson2t <- lawson2[-c(1:57), ]
#correct first initials
 lawson2t$initials[1:9] <- "ML:"
 lawson2t$initials[10:11] <- "RS:"
 lawson2t$initials[12:13] <- "EL:"
#remove full names
 lawson2t$text[10] <- gsub("Rita Swidrowski:", "",  lawson2t$text[10])
 lawson2t$text[12] <- gsub("Edwin Lawson:", "",  lawson2t$text[12])
#remove initials from text
lawson2t$text <- str_replace_all(lawson2t$text, "[A-Z]{1,3}:", "")
#remove white spaces
lawson2t$text <- gsub("\\s{2,}", "", lawson2t$text)
#remove empty rows
lawson2t <- lawson2t[!is.na(lawson2t$text), ]
#merging text
lawson2t <- lawson2t %>%
  mutate(group = cumsum(initials != lag(initials, default = "")))
#merge consecutive rows within each group to have
lawson2t_merged <- lawson2t %>%
  group_by(initials, group) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup() %>% 
  arrange(group)
#extract time stamps
lawson2t_merged <- lawson2t_merged %>%
  mutate(time = str_extract_all(text, "\\[\\d{2}:\\d{2}:\\d{2}\\.\\d{2}\\]"))
time_unnested <- lawson2t_merged %>%
  unnest(time)
#remove time stamps from text
lawson2t_merged$text <- gsub("\\[\\d{2}:\\d{2}:\\d{2}\\.\\d{2}\\]", "",lawson2t_merged$text)
```

## TF-IDF (EL1 and EL2)
```{r term_frequency_EL2}
lawson2_word <- lawson2t %>%
  unnest_tokens(word, text) 

lawson2_word <- lawson2_word %>%
  anti_join(get_stopwords(source = "smart")) %>% 
  count(word, sort = TRUE)

word_document_freq_2 <- lawson2_word %>%
  mutate(tf = n / sum(n)) 

print(word_document_freq_2)
```

```{r tf_idf_interviews}
lawson1t <- lawson1t %>% 
  mutate(interview = "EL1")
lawson2t <- lawson2t %>% 
  mutate(interview = "EL2")

EL_interviews <- bind_rows(lawson1t, lawson2t)

interviews_word <- EL_interviews %>%
  unnest_tokens(word, text) %>% 
  anti_join(get_stopwords(source = "smart")) %>% 
  count(interview, word, sort = TRUE)

interviews_tf_idf <- interviews_word %>%
  bind_tf_idf(word, interview, n)

interviews_tf_idf %>%
  arrange(-tf_idf)

```


