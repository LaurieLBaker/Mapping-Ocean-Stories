---
title: "Edwin Lawson Interview Analysis"
output: html_document
date: "2023-07-18"
name: "Delphine Demaisy"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r install_packages}
library(pdftools)
library(tidyverse)
library(tidytext)
library(dplyr)
library(textclean)
library(stopwords)
# install.packages("spacyr")
# library(spacyr)
# spacy_download_langmodel("en") #need to install Anaconda on computer (not currently working)
# spacy_initialize("en")
```

## Reading in the data
```{r read_data}
lawson1_full_text <- pdftools::pdf_text(pdf = "data/Lawson_Edwin_10131972.pdf")
```

```{r string-data}
# convert text to string
lawson1_full_text <- toString(lawson1_full_text)
lawson1_full_text
# convert text to character lines
lawson1 <- read_lines(lawson1_full_text)

lawson1 <- tibble(lawson1)
lawson1 <- rename(lawson1, text = lawson1)
```

## Cleaning table
```{r data_cleaning}
lawson1 <- lawson1 %>% 
  mutate(initials = str_extract(text, pattern = "[A-Z]{1,3}\\:")) %>%
  fill(initials, .direction = "down")
#remove the metadata and only look at interview
lawson1t <- lawson1[-c(1:60), ]
#correct first initials
lawson1t$initials[1:4] <- "RS:"
lawson1t$initials[5:7] <- "EL:"
#remove initials from text
lawson1t$text <- str_replace_all(lawson1t$text, "[A-Z]{1,3}:", "")
#remove white spaces
lawson1t$text <- gsub("\\s{2,}", "", lawson1t$text)
#remove empty rows
lawson1t <- lawson1t[!is.na(lawson1t$text), ]
#merging text
lawson1t <- lawson1t %>%
  mutate(group = cumsum(initials != lag(initials, default = "")))
#merge consecutive rows within each group to have
lawson1t_merged <- lawson1t %>%
  group_by(initials, group) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup() %>% 
  arrange(group)
#extract time stamps
lawson1t_merged <- lawson1t_merged %>%
  mutate(time = str_extract_all(text, "\\[\\d{2}:\\d{2}:\\d{2}\\.\\d\\]"))
time_unnested <- lawson1t_merged %>%
  unnest(time)
#do we want to remove the time stamps from the text?
```

##SpacyR location extraction
```{r}
# parsed_text <- spacy_parse(lawson1t_merged$text)
# location_entities <- spacy_entity(parsed_text, entity = "GPE")
# locations <- location_entities$ent_text
```
##Location extraction from list
```{r location_function}
# List of locations from ChatGPT
locations <- c("West Tremont", "home", "Seal Cove", "Ship Harbor", "Tremont",
               "Rockland", "Maine", "Swan's Island", "Bass Harbor", "Blue Hill Bay",
               "Hardwood Island", "Stonington", "Blue Hill", "Black Island", "Goose Cove")
#function to extract location
extract_location <- function(text) 
  { extracted_location <- str_extract_all(text, paste(locations, collapse = "|"))
  return(extracted_location)}
#link to lawson1t_merged
lawson1t_merged$location <- sapply(lawson1t_merged$text, extract_location)
```
##Word frequency
```{r word_frequency_plot}
lawson1_word <- lawson1t %>% 
  unnest_tokens(word, text) 
lawson1_word %>% 
  anti_join(get_stopwords(source = "smart")) %>% 
  count(word, sort = TRUE) %>%
  slice_max(n, n = 20) %>%
  ggplot(aes(n, fct_reorder(word, n))) + 
  geom_col()
#see comments about this in Trello
```

