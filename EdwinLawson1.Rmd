---
title: "Edwin Lawson Interview Analysis"
output: html_document
date: "2023-07-18"
name: "Delphine Demaisy"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r install_packages}
library(pdftools)
library(tidyverse)
library(tidytext)
library(dplyr)
library(textclean)
library(stopwords)
library(ggplot2) #for geom_segment location
library(ggraph) #for geom edge link location
library(wordcloud) #for wordcloud
library(RColorBrewer) #for wordcloud
library(topicmodels) #for LDA topic modelling
library(reshape2) #for LDA topic modelling
library(igraph) #for network graph
library(shiny) #for shiny app
library(data.table) # to modify data.table
# install.packages("spacyr")
# library(spacyr)
# spacy_download_langmodel("en") #need to install Anaconda on computer (not currently working)
# spacy_initialize("en")
```

## Reading in the data EL1
```{r read_data}
lawson1_full_text <- pdftools::pdf_text(pdf = "data/Lawson_Edwin_10131972.pdf")
```

```{r string-data}
# convert text to string
lawson1_full_text <- toString(lawson1_full_text)
lawson1_full_text
# convert text to character lines
lawson1 <- read_lines(lawson1_full_text)

lawson1 <- tibble(lawson1)
lawson1 <- rename(lawson1, text = lawson1)
```

## Cleaning table EL1

```{r data_cleaning}
lawson1 <- lawson1 %>% 
  mutate(initials = str_extract(text, pattern = "[A-Z]{1,3}\\:")) %>%
  fill(initials, .direction = "down")
#remove the metadata and only look at interview
lawson1t <- lawson1[-c(1:60), ]
#correct first initials
lawson1t$initials[1:4] <- "RS:"
lawson1t$initials[5:7] <- "EL:"
#remove initials from text
lawson1t$text <- str_replace_all(lawson1t$text, "[A-Z]{1,3}:", "")
#remove white spaces
lawson1t$text <- gsub("\\s{2,}", "", lawson1t$text)
#remove empty rows
lawson1t <- lawson1t[!is.na(lawson1t$text), ]
#merging text
lawson1t <- lawson1t %>%
  mutate(group = cumsum(initials != lag(initials, default = "")))
#merge consecutive rows within each group to have
lawson1t_merged <- lawson1t %>%
  group_by(initials, group) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup() %>% 
  arrange(group)
#extract time stamps
lawson1t_merged <- lawson1t_merged %>%
  mutate(time = str_extract_all(text, "\\[\\d{2}:\\d{2}:\\d{2}\\.\\d\\]"))
time_unnested <- lawson1t_merged %>%
  unnest(time)
#remove time stamps from text
lawson1t_merged$text <- gsub("\\[\\d{2}:\\d{2}:\\d{2}\\.\\d\\]", "",lawson1t_merged$text)
```

## Extracting keyword + words of the same sentence before and after EL1(need to figure out how to deal with multiple mentions in the same row)

```{r extract_keyword}
# Choose a  keyword
new_keyword <- "traps"  # change keyword here

# Function to extract keyword and words before/after it
extract_keyword_and_context <- function(text, keyword) {
  keyword_indices <- gregexpr(paste0("(?i)\\b", keyword, "\\b"), text)
  num_occurrences <- sum(keyword_indices[[1]] != -1)
  
  if (num_occurrences == 0) {
    return(list(before = NULL, after = NULL, count = num_occurrences, keyword_occurrences = ""))
  }
  
  keyword_occurrences <- unlist(regmatches(text, keyword_indices))
  
   before <- sub(".*[.!?] ", "", substr(text, 1, min(keyword_indices[[1]] + attr(keyword_indices[[1]], "match.length")) - 1))
  before <- gsub(paste0("(?i)\\b", keyword, "\\b"), "", before)  
  after <- sub(paste0(".*(?i)\\b", keyword, "\\b(.*?[.!?])"), "\\1", text)
  
  return(list(keyword = keyword, before = before, after = after, count = num_occurrences, keyword_occurrences = keyword_occurrences))
}

# Function to process data with a given keyword
process_data_with_keyword <- function(data, keyword) {
  # Extract keyword and context for each row in the dataframe
  processed_data <- data %>%
    rowwise() %>%
    mutate(keyword_context = list(extract_keyword_and_context(text, keyword)))
  
  # Unnest the keyword_context column to get separate before and after columns
  processed_data <- processed_data %>%
    unnest_wider(c(keyword_context)) %>%
    mutate(
      before = coalesce(before, ""),
      after = coalesce(after, ""),
      count_keyword = coalesce(count, 0)
    ) %>% 
    select(initials, group, text, time, before, keyword_occurrences, after, count_keyword)
  
  return(processed_data)
}

# Process data with the new keyword
lawson1t_merged_keyword <- process_data_with_keyword(lawson1t_merged, new_keyword)

#Pull part of text to verify accuracy/see whole entry
lawson1t_merged_keyword %>% 
  filter(group == 54) %>%
  pull(text)
```

## SpacyR location extraction EL1 

### (I believe we need to also have Python installed on computer with the SpaCy for spacyr to work. I don't have it, anyone does?)

```{r spacryr_location_extraction}
# parsed_text <- spacy_parse(lawson1t_merged$text)
# location_entities <- spacy_entity(parsed_text, entity = "GPE")
# locations <- location_entities$ent_text
```

## Location extraction from list and ChatGPT EL1

```{r location_list_chatgpt}
# List of locations from ChatGPT with EL interview 1 only
locations <- c("West Tremont", "home", "Seal Cove", "Ship Harbor", "Tremont",
               "Rockland", "Maine", "Swan's Island", "Bass Harbor", "Blue Hill Bay",
               "Hardwood Island", "Stonington", "Blue Hill", "Black Island", "Goose Cove", "England")
```

```{r location_list_EL_compiled}
# List of locations compiled by us while coding all the EL interviews
locations_EL <- c("Swan's Island", "home", "Southwest Harbor", "Manset", "Worcester", "Massachusetts", "California", "Pasadena", "Florida", "Lake Worth", "Palm Beach", "Westward", "Cherryfield", "Canada", "Hancock", "Bangor", "Castine", "Ship Harbor", "West", "Boston", "Gloucester", "New York", "Black Island", "Mitchell's Cove", "Placentia", "Bass Harbor", "Blue Hill", "Blue Hill Bay", "Hardwood Island", "Stonington", "Seal Cove", "Mitchellâ€™s Cove", "Goose Cove", "West Tremont", "Lynn", "Marblehead", "Maine", "Bass Harbor Head", "Bear Island", "Ship Island Ledges", "Ship Island", "bay", "Gulf", "Wilson", "Bernard", "Portland", "Rockland", "Mt. Desert Island/ Mount Desert Island", "Chester", "England", "Tremont", "Ellsworth", "Cranberry Island", "Mt. Desert Rock/ Mount Desert Rocks", "Duck Island", "Duck Cove", "Cape Cod", "Camden", "Norway", "Saugus", "Nova Scotia", "Atlantic", "Brooklin", "Corea", "Prospect Harbor", "Orono", "New Hampshire", "Hancock county", "Bar Harbor", "Northeast Harbor", "Vinalhaven", "Eastward", "Oak Point", "Long Ledge", "Platt's Point", "Somesville", "England")
```

```{r function_location_list}
#function to extract location
extract_location <- function(text) 
  { extracted_location <- str_extract_all(text, paste(locations, collapse = "|"))
  return(extracted_location)}
#link to lawson1t_merged
lawson1t_merged$location <- sapply(lawson1t_merged$text, extract_location)
#check how many picked out by chatGPT --> 21 rows with locations
lawson1t_merged %>% 
  count(location) %>% 
  summarize(sum = location)
```
```{r function_location_EL_list}
# #function to extract locations from compiled EL list
extract_locations_EL <- function(text)
  { extracted_locations_EL <- str_extract_all(text, paste(locations_EL, collapse = "|"))
  return(extracted_locations_EL)}
# #link to lawson1t_merged
lawson1t_merged$location <- sapply(lawson1t_merged$text, extract_locations_EL)
#check how many picked out by chatGPT --> 21 rows with locations
lawson1t_merged %>%
  count(location) %>%
  summarize(sum = location)
#this gives 5 more locations than the list from ChatGPT. It separated West Tremont into two like West and Tremont. These have been mentioned both together and as separate. The locations were still picked out wich means that ChatGPT was still accurate. However by only using ChatGPT we might also miss a few.
```

## Location network EL1

```{r location_network_trial1, fig.height=15, fig.width=15}
#This one is not working (might want to use this template for locations and their keywords associated to it)
# Filter for the rows with locations
filtered_merged <- lawson1t_merged %>%
  filter(location != "character(0)") %>% 
  unnest(location)

# Add unique identifiers to location names
filtered_merged$location <- paste0(filtered_merged$location, "_", filtered_merged$group)

# Create list of connection locations (within the same group)
connection_list_1 <- filtered_merged %>%
  group_by(group) %>%
  filter(n() > 1) %>%
  mutate(from = lag(location), to = lead(location)) %>%
  na.omit() %>%
  select(from, to)

# Create a graph from the filtered data
network_location_1 <- graph_from_data_frame(connection_list_1, directed = TRUE)

# Get unique initials
initials <- unique(filtered_merged$initials)

# Choose colors
colors <- ifelse(grepl("^RS:", initials), "red", "blue")

# Plot network
plot(network_location_1, vertex.color = colors)
```
```{r network_location_trial2}
#This one is not working
# Filter for the rows with locations
filtered_merged <- lawson1t_merged %>%
  filter(location != "character(0)") %>% 
  unnest(location)

# Add unique identifiers to location names
#filtered_merged$location <- paste0(filtered_merged$location, "_", filtered_merged$group)

# Create list of connection locations (within the same group)
connection_list_2 <- filtered_merged %>%
  group_by(group, initials) %>%
  filter(n() > 1) #%>%
#  mutate(from = paste0(location, "_", initials), to = paste0(lead(location), "_", lead(initials))) %>%
  # na.omit() %>%
  # select(from, to)

# Create a graph from the filtered data
network_location_2 <- graph_from_data_frame(d = connection_list_2, directed = TRUE)

# # Get unique initials
# initials <- unique(filtered_merged$initials)

# Assign unique vertex names
# unique_names <- paste0(V(network_location)$name, "_", sequence(vcount(network_location)))
# V(network_location)$name <- unique_names

# Choose colors
colors <- ifelse(grepl("^RS:", initials), "red", "blue")

# Plot network with vertex color based on initials
plot(network_location_2, vertex.color = colors[V(network_location_2)$initials])
```

```{r geom_edge_link_location, fig.height=12, fig.width=30}
# Mostly working (good enough for now)
connection_list_3 <- connection_list_2 %>%
  group_by(group) %>%
  summarize(combination = list(combn(location, 2, simplify = FALSE))) %>%
  unnest(combination) %>%
  separate(combination, into = c("from", "to"), sep = ", ", convert = TRUE) %>% 
  mutate(from = gsub("^c\\(|\\)$|\"|'", "", from),
         to = gsub("^c\\(|\\)$|\"|'", "", to))

# Calculate the number of connections for each node (to display the size accordingly)
node_connections <- connection_list_3 %>%
  count(from, name = "connections") %>%
  bind_rows(connection_list_3 %>% count(to, name = "connections")) %>%
  group_by(from) %>%
  summarize(max_connections = max(connections, na.rm = TRUE))

# Merge node_degrees
connection_list_3 <- left_join(connection_list_3, node_connections, by = c("from" = "from"))

# Combine the 'group' and '(from, to)' variables to create unique ids
connection_list_3 <- connection_list_3 %>%
  mutate(unique_id = interaction(group, from, to))

connection_list_3 %>%
  ggraph(layout = "nicely") + #see other layout options
  geom_edge_link(aes(alpha = 2, 
                     color = unique_id),  #to color by group
                 show.legend = TRUE, #put to false to only have colors displayed but not legend
                 arrow = arrow(length = unit(1.5, "mm")),
                 start_cap = circle(3, "mm"),
                 end_cap = circle(3, "mm")) +
  geom_node_text(aes(label = name),
                 size = 6) +  # size by max_connection doesn't work here
  scale_size_continuous(range = c(6, 12), 
                        guide = "legend") +  # legend of size not working
  scale_color_viridis_d() +
  # theme(legend.direction = "vertical", 
  #       legend.text = element_text(size = 12), 
  #       legend.key.width = unit(2, "cm")) + #this formatting of the color legend is also not working
  theme_graph() 
```


## Gear Extraction from list ChatGPT EL1

```{r gear_list}
gear <- c("tape recorder", "battery", "sardine weir", "fishing lines", "lobster traps", "rowboat", "lobster smack", "lobster buyers", "lobster companies", "lobster boat", "lobster catcher", "outboard motor", "two and a half horsepower outboard motor", "wooden lobster traps", "softwood traps", "second-hand traps", "laths", "boughs", "bottoms", "frames", "nylon twine", "cotton twine", "manila twine", "polyethylene twine", "plastic buoys", "foam buoys", "styrofoam buoys", "wooden buoys", "cedar wood", "oak wood", "spruce wood", "pot warp", "toggle", "single-cylinder boat engine", "four-cylinder boat engine", "six-cylinder boat engine", "eight-cylinder boat engine", "diesel engine", "gasoline engine", "hundred and thirty-five horsepower gasoline engine", "eighty-horsepower diesel engine","seine net")
```

```{r lower_case_text}
#we need all the text to be in lower case and the list to be in lower case to extract all the gear. However we need to keep the Uppercase when we want to extract Location, which is why the location extraction should stay first.
lawson1t_merged_lower <- lawson1t_merged %>%
  mutate(text = tolower(text))
```

```{r function_gear_list}
#function to extract gear
extract_gear <- function(text) 
  { extracted_gear <- str_extract_all(text, paste(gear, collapse = "|"))
  return(extracted_gear)}
#link to lawson1t_merged_lower
lawson1t_merged_lower$gear <- sapply(lawson1t_merged_lower$text, extract_gear)
```

## Species Extraction from list ChatGPT EL1

```{r species_list}
species <- c("lobsters", "scallops", "lobster", "scallop", "sardine", "sardines")
```

```{r species_list_function}
#function to extract species
extract_species <- function(text) 
  { extracted_species <- str_extract_all(text, paste(species, collapse = "|"))
  return(extracted_species)}
#link to lawson1t_merged_lower
lawson1t_merged_lower$species <- sapply(lawson1t_merged_lower$text, extract_species)
```


## Explore Data.Table Packages with version of lawson1t_merged_lower (extraction of location, gear, species)

### It is possible in R to change the content of the cells trough simple code, but we NEED shiny to be able to have someone modify the content of the cells and add their initials directly to the data table. Then it is also possible to make a copy of the original data table before it is modified.

```{r data.table}
#keep in mind that data.table modifies the data directly!!!

lawson_to_modify <- data.table(lawson1t_merged_lower)
```

## Explore Text Wrapping

### Doesn't seem to be working super well in R directly, but ChatGPT says that it will be more useful within the shiny App --> we will probably see results there.
```{r text-wrapping}
# Convert to data.table
setDT(lawson1t_merged_lower)

# Function to wrap the text and create a new data.table with individual wrapped lines
wrap_text_lines <- function(text, width) {
  wrapped_lines <- strwrap(text, width = width)
  data.table(Wrappedtext = unlist(wrapped_lines))
}

# Wrap the text in the "text" column and store the result in the new data.table
width <- 1000  # Adjust the width as needed
wrap_lawson1t_merged_lower <- lawson1t_merged_lower[, wrap_text_lines(text, width), by = group]

# Print the modified data.table with wrapped text
print(wrap_lawson1t_merged_lower)
```

## Word frequency EL1

```{r word_frequency_plot}
lawson1_word <- lawson1t %>% 
  unnest_tokens(word, text) 
lawson1_word %>% 
  anti_join(get_stopwords(source = "smart")) %>% 
  count(word, sort = TRUE) %>%
  slice_max(n, n = 20) %>%
  ggplot(aes(n, fct_reorder(word, n))) + 
  geom_col()
#see comments about this in Trello
```

## TF EL1

```{r term_frequency}
lawson1_word <- lawson1t %>%
  unnest_tokens(word, text) 

lawson1_word <- lawson1_word %>%
  anti_join(get_stopwords(source = "smart")) %>%
  count(word, sort = TRUE)

word_document_freq_1 <- lawson1_word %>%
  mutate(tf = n / sum(n)) 

print(word_document_freq_1)
```

## Word cloud list

```{r list_frequencies_wordcloud, fig.height=8, fig.width=8}
#words provided by ChatGPT + added bay, summer, harbor, dollars from the word frequency plot above. (total of 37 words --> decide what amount of words would be best) I also added weiring and dragging (they are not of high frequency, but it is mentionned, which would be good to portray)
word_list <- c("lobster", "boat", "traps", "fishermen", "water", "lobstering", "oil", "clothes",
               "motor", "work", "years", "fishing", "rowboat", "wharf", "season", "boats",
               "diesel", "buoys", "accidents", "licence", "gasoline", "engine", "swim", "son",
               "horsepower", "fishing", "heavy", "boots", "color", "land", "coast", "shore",
               "family", "winter", "father", "summer", "bay", "harbor", "dollars", "weiring", "dragging")
words_freq_1 <- lawson1_word
words_freq_1 <- words_freq_1[words_freq_1$word %in% word_list, ]
color_palette <- brewer.pal(8, "Set2")
wordcloud(
  words_freq_1$word,  
  words_freq_1$n,
  colors = color_palette,
  random.order = FALSE,  # To keep the word order as per frequency
  scale = c(5, 0.5),      
  max.words = 50,
)
```

## Topic Modeling 

###(is not very conclusive for now, perhaps adding more documents (more interviews) could make the topic modelling more relevant?)

```{r LDA_topic_modeling}
lawson1_word <- lawson1_word  %>% 
  anti_join(get_stopwords(source = "smart")) %>% 
  mutate(document_id = row_number())
#create document term matrix (DTM)
dtm <- lawson1_word %>%
  count(document_id, word) %>%
  cast_dtm(document_id, word, n)
#set number of topics
num_topics <- 20
#run Latent Dirichlet Allocation (LDA) topic modeling algorithm 
lda_model <- LDA(dtm, k = num_topics)
#most probable words for each topic
terms_per_topic <- terms(lda_model, 10)
#get most dominant topic for each document
document_topics <- tidy(lda_model, matrix = "gamma") %>%
  group_by(document) %>%
  top_n(1, gamma)
```
## Reading in the data EL2
```{r read_data_EL2}
lawson2_full_text <- pdftools::pdf_text(pdf = "data/Lawson_Edwin_10201972.pdf")
```

```{r string-data_EL2}
# convert text to string
lawson2_full_text <- toString(lawson2_full_text)
lawson2_full_text
# convert text to character lines
lawson2 <- read_lines(lawson2_full_text)

lawson2 <- tibble(lawson2)
lawson2 <- rename(lawson2, text = lawson2)
```

## Cleaning table EL2
```{r data_cleaning_EL2}
lawson2 <- lawson2 %>% 
  mutate(initials = str_extract(text, pattern = "[A-Z]{1,3}\\:")) %>%
  fill(initials, .direction = "down")
#remove the metadata and only look at interview
lawson2t <- lawson2[-c(1:57), ]
#correct first initials
 lawson2t$initials[1:9] <- "ML:"
 lawson2t$initials[10:11] <- "RS:"
 lawson2t$initials[12:13] <- "EL:"
#remove full names
 lawson2t$text[10] <- gsub("Rita Swidrowski:", "",  lawson2t$text[10])
 lawson2t$text[12] <- gsub("Edwin Lawson:", "",  lawson2t$text[12])
#remove initials from text
lawson2t$text <- str_replace_all(lawson2t$text, "[A-Z]{1,3}:", "")
#remove white spaces
lawson2t$text <- gsub("\\s{2,}", "", lawson2t$text)
#remove empty rows
lawson2t <- lawson2t[!is.na(lawson2t$text), ]
#merging text
lawson2t <- lawson2t %>%
  mutate(group = cumsum(initials != lag(initials, default = "")))
#merge consecutive rows within each group to have
lawson2t_merged <- lawson2t %>%
  group_by(initials, group) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup() %>% 
  arrange(group)
#extract time stamps
lawson2t_merged <- lawson2t_merged %>%
  mutate(time = str_extract_all(text, "\\[\\d{2}:\\d{2}:\\d{2}\\.\\d{2}\\]"))
time_unnested <- lawson2t_merged %>%
  unnest(time)
#remove time stamps from text
lawson2t_merged$text <- gsub("\\[\\d{2}:\\d{2}:\\d{2}\\.\\d{2}\\]", "",lawson2t_merged$text)
```

## TF-IDF (EL1 and EL2)
```{r term_frequency_EL2}
lawson2_word <- lawson2t %>%
  unnest_tokens(word, text) 

lawson2_word <- lawson2_word %>%
  anti_join(get_stopwords(source = "smart")) %>% 
  count(word, sort = TRUE)

word_document_freq_2 <- lawson2_word %>%
  mutate(tf = n / sum(n)) 

print(word_document_freq_2)
```
```{r tf-idf_EL1_EL2}
#do we perhaps also want to create a running list of our own stop_words to add so that we can remove them and they won't show up here (eg: that's, there's, they're, can't)
lawson1t <- lawson1t %>% 
  mutate(interview = "EL1")

lawson2t <- lawson2t %>% 
  mutate(interview = "EL2")

EL_interviews <- bind_rows(lawson1t, lawson2t)

interviews_word <- EL_interviews %>%
  unnest_tokens(word, text) %>% 
  anti_join(get_stopwords(source = "smart")) %>% 
  count(interview, word, sort = TRUE)

interviews_tf_idf <- interviews_word %>%
  bind_tf_idf(word, interview, n)
interviews_tf_idf %>%
  arrange(-tf_idf)
```
### Word Frequency Plot TF-IDF (EL1 and EL2)

```{r tf-idf_word_frequency_viz, fig.height=8, fig.width=5}
interviews_tf_idf %>%
  group_by(interview) %>%
  slice_max(tf_idf, n = 20) %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = interview)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~interview, scales = "free")
```

### WordCloud TF-IDF (EL1 and EL2)

```{r word_cloud_tf-idf, fig.height=8, fig.width=8}
wordcloud(
  words = interviews_tf_idf$word,
  freq = interviews_tf_idf$tf_idf,
  scale = c(5, 0.5),
  colors = brewer.pal(length(unique(interviews_tf_idf$interview)), "Set2"),
  max.words = 40,
  random.order = FALSE
)
legend("topright", legend = unique(interviews_tf_idf$interview), fill = colors)
#Warning: minimal value for n is 3, returning requested palette with 3 different levels
```

