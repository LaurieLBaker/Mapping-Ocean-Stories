---
title: "Edwin Lawson Interview Analysis"
output: html_document
date: "2023-07-18"
name: "Delphine Demaisy"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r install_packages}
library(pdftools)
library(tidyverse)
library(tidytext)
library(dplyr)
library(textclean)
library(stopwords)
library(wordcloud) #for wordcloud
library(RColorBrewer) #for wordcloud
library(topicmodels) #for LDA topic modelling
library(reshape2) #for LDA topic modelling
library(igraph) #for network graph
# install.packages("spacyr")
# library(spacyr)
# spacy_download_langmodel("en") #need to install Anaconda on computer (not currently working)
# spacy_initialize("en")
```

## Reading in the data
```{r read_data}
lawson1_full_text <- pdftools::pdf_text(pdf = "data/Lawson_Edwin_10131972.pdf")
```

```{r string-data}
# convert text to string
lawson1_full_text <- toString(lawson1_full_text)
lawson1_full_text
# convert text to character lines
lawson1 <- read_lines(lawson1_full_text)

lawson1 <- tibble(lawson1)
lawson1 <- rename(lawson1, text = lawson1)
```

## Cleaning table
```{r data_cleaning}
lawson1 <- lawson1 %>% 
  mutate(initials = str_extract(text, pattern = "[A-Z]{1,3}\\:")) %>%
  fill(initials, .direction = "down")
#remove the metadata and only look at interview
lawson1t <- lawson1[-c(1:60), ]
#correct first initials
lawson1t$initials[1:4] <- "RS:"
lawson1t$initials[5:7] <- "EL:"
#remove initials from text
lawson1t$text <- str_replace_all(lawson1t$text, "[A-Z]{1,3}:", "")
#remove white spaces
lawson1t$text <- gsub("\\s{2,}", "", lawson1t$text)
#remove empty rows
lawson1t <- lawson1t[!is.na(lawson1t$text), ]
#merging text
lawson1t <- lawson1t %>%
  mutate(group = cumsum(initials != lag(initials, default = "")))
#merge consecutive rows within each group to have
lawson1t_merged <- lawson1t %>%
  group_by(initials, group) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup() %>% 
  arrange(group)
#extract time stamps
lawson1t_merged <- lawson1t_merged %>%
  mutate(time = str_extract_all(text, "\\[\\d{2}:\\d{2}:\\d{2}\\.\\d\\]"))
time_unnested <- lawson1t_merged %>%
  unnest(time)
#do we want to remove the time stamps from the text?
```

##SpacyR location extraction (I believe we need to also have Python installed on computer with the SpaCy for spacyr to work. I don't have it, anyone does?)
```{r spacryr_location_extraction}
# parsed_text <- spacy_parse(lawson1t_merged$text)
# location_entities <- spacy_entity(parsed_text, entity = "GPE")
# locations <- location_entities$ent_text
```
##Location extraction from list
```{r location_list_chatgpt}
# List of locations from ChatGPT with EL interview 1 only
locations <- c("West Tremont", "home", "Seal Cove", "Ship Harbor", "Tremont",
               "Rockland", "Maine", "Swan's Island", "Bass Harbor", "Blue Hill Bay",
               "Hardwood Island", "Stonington", "Blue Hill", "Black Island", "Goose Cove")
```

```{r location_list_EL_compiled}
# List of locations compiled by us while coding all the EL interviews
locations_EL <- c("Swan's Island", "home", "Southwest Harbor", "Manset", "Worcester", "Massachusetts", "California", "Pasadena", "Florida", "Lake Worth", "Palm Beach", "Westward", "Cherryfield", "Canada", "Hancock", "Bangor", "Castine", "Ship Harbor", "West", "Boston", "Gloucester", "New York", "Black Island", "Mitchell's Cove", "Placentia", "Bass Harbor", "Blue Hill", "Blue Hill Bay", "Hardwood Island", "Stonington", "Seal Cove", "Mitchellâ€™s Cove", "Goose Cove", "West Tremont", "Lynn", "Marblehead", "Maine", "Bass Harbor Head", "Bear Island", "Ship Island Ledges", "Ship Island", "bay", "Gulf", "Wilson", "Bernard", "Portland", "Rockland", "Mt. Desert Island/ Mount Desert Island", "Chester", "England", "Tremont", "Ellsworth", "Cranberry Island", "Mt. Desert Rock/ Mount Desert Rocks", "Duck Island", "Duck Cove", "Cape Cod", "Camden", "Norway", "Saugus", "Nova Scotia", "Atlantic", "Brooklin", "Corea", "Prospect Harbor", "Orono", "New Hampshire", "Hancock county", "Bar Harbor", "Northeast Harbor", "Vinalhaven", "Eastward", "Oak Point", "Long Ledge", "Platt's Point", "Somesville")
```

```{r function_location_list}
#function to extract location
extract_location <- function(text) 
  { extracted_location <- str_extract_all(text, paste(locations, collapse = "|"))
  return(extracted_location)}
#link to lawson1t_merged
lawson1t_merged$location <- sapply(lawson1t_merged$text, extract_location)
#check how many picked out by chatGPT --> 21 rows with locations
lawson1t_merged %>% 
  count(location) %>% 
  summarize(sum = location)
```
```{r function_location_EL_list}
# #function to extract locations from compiled EL list
# extract_locations_EL <- function(text) 
#   { extracted_locations_EL <- str_extract_all(text, paste(locations, collapse = "|"))
#   return(extracted_locations_EL)}
# #link to lawson1t_merged
# lawson1t_merged$location <- sapply(lawson1t_merged$text, extract_locations_EL)
# #check how many picked out by chatGPT --> 21 rows with locations
# lawson1t_merged %>% 
#   count(location) %>% 
#   summarize(sum = location)
# #this gives the same amount of results with the big as the one gotten from ChapGPT = chaGPT reliable
```

##Location network
```{r location_network}
#filter for the rows with locations
filtered_merged <- lawson1t_merged %>%
  filter((location != "character(0)")) %>% 
  unnest(location)
#create list of connection locations (with consecutive initials only)
# connection_list <- filtered_merged %>%
#   mutate(initials = as.character(initials)) %>%
#   group_by(grp = cumsum(c(TRUE, diff(as.integer(factor(initials))) != 1))) %>%
#   filter(n() > 1) %>%
#   mutate(from = lag(location), to = lead(location)) %>%
#   select(from, to)
# 
# network_location <- graph_from_data_frame(filtered_merged, directed = TRUE)
# #
# initials <- unique(filtered_merged$initials)
# colors <- rainbow(length(initials))
# names(colors) <- initials
# plot(network_location, vertex.color = colors)
```

##Word frequency
```{r word_frequency_plot}
lawson1_word <- lawson1t %>% 
  unnest_tokens(word, text) 
lawson1_word %>% 
  anti_join(get_stopwords(source = "smart")) %>% 
  count(word, sort = TRUE) %>%
  slice_max(n, n = 20) %>%
  ggplot(aes(n, fct_reorder(word, n))) + 
  geom_col()
#see comments about this in Trello
```
###Word cloud list
```{r list_frequencies_wordcloud}
#words provided by ChatGPT + added bay, summer, harbor, dollars from the word frequency plot above. (total of 37 words --> decide what amount of words would be best) I also added weiring and dragging (they are not of high frequency, but it is mentionned, which would be good to portray)
word_list <- c("lobster", "boat", "traps", "fishermen", "water", "lobstering", "oil", "clothes",
               "motor", "work", "years", "fishing", "rowboat", "wharf", "season", "boats",
               "diesel", "buoys", "accidents", "licence", "gasoline", "engine", "swim", "son",
               "horsepower", "fishing", "heavy", "boots", "color", "land", "coast", "shore",
               "family", "winter", "father", "summer", "bay", "harbor", "dollars", "weiring", "dragging")
word_freq <- table(lawson1_word$word)
target_freq <- word_freq[names(word_freq) %in% word_list]
target_freq <- sort(target_freq, decreasing = TRUE)
```

```{r word_cloud}
wordcloud(
  words = names(target_freq),
  freq = target_freq,
  min.freq = 1,
  scale = c(5, 1), #Size range for the words in the cloud (big, small)
  random.order = FALSE, #FALSE will put the biggest words in the middle, TRUE will put them in a random order
  colors = brewer.pal(8, "Dark2") #(amount of colors, palette)
)
```

##Topic Modeling (is not very conclusive for now, perhaps adding more documents (more interviews) could make the topic modelling more relevant?)
```{r LDA_topic_modeling}
lawson1_word <- lawson1_word  %>% 
  anti_join(get_stopwords(source = "smart")) %>% 
  mutate(document_id = row_number())
#create document term matrix (DTM)
dtm <- lawson1_word %>%
  count(document_id, word) %>%
  cast_dtm(document_id, word, n)
#set number of topics
num_topics <- 20
#run Latent Dirichlet Allocation (LDA) topic modeling algorithm 
lda_model <- LDA(dtm, k = num_topics)
#most probable words for each topic
terms_per_topic <- terms(lda_model, 10)
#get most dominant topic for each document
document_topics <- tidy(lda_model, matrix = "gamma") %>%
  group_by(document) %>%
  top_n(1, gamma)
```

