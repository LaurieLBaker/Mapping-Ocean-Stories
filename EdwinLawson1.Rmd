---
title: "Edwin Lawson Interview Analysis"
output: html_document
date: "2023-07-18"
name: "Delphine Demaisy"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Install the various packages & libraries

```{r install_packages}
library(pdftools)
library(tidyverse)
library(tidytext)
library(dplyr)
library(textclean)
library(stopwords)
library(ggplot2) #for geom_segment location
library(ggraph) #for geom edge link location
library(wordcloud) #for wordcloud
library(RColorBrewer) #for wordcloud
library(topicmodels) #for LDA topic modelling
library(reshape2) #for LDA topic modelling
library(igraph) #for network graph
library(shiny) #for shiny app
library(data.table) # to modify data.table
```

#### Reading in the data of the first Edwin Lawson Interview (10131972) from pdf and create lawson1_full_text
```{r read_data}
lawson1_full_text <- pdftools::pdf_text(pdf = "data/Lawson_Edwin_10131972.pdf")
```

#### Create a lawson1, a tibble to contain the text as character lines
```{r string-data}
# convert text to string
lawson1_full_text <- toString(lawson1_full_text)
lawson1_full_text
# convert text to character lines
lawson1 <- read_lines(lawson1_full_text)

lawson1 <- tibble(lawson1)
lawson1 <- rename(lawson1, text = lawson1)
```

#### Cleaning the lawson1 table by extracting creating a column for initials and extracting the initials from the text. Then remove the metadata (first two pages of general information about the interview) to only have a table containing text, it is called lawson1t. With Lawson1t, correct the first few row to include the right initials in the initials column, then remove these initials from the text. Remove white spaces in the text and all empty rows. Then merge the rows that have the same consecutive initials to get the whole chunk of text one person was saying in a single row, this dataset becomes Lawson1t_merged. Create the group column to keep track of the oder of people talking in the interview. Extract the time stamps from the text and put them in a time column, then remove the time stamps from the text column.

```{r data_cleaning}
lawson1 <- lawson1 %>% 
#extract initials
  mutate(initials = str_extract(text, pattern = "[A-Z]{1,3}\\:")) %>%
  fill(initials, .direction = "down")
#remove the metadata and only look at interview
lawson1t <- lawson1[-c(1:60), ]
#correct first initials
lawson1t$initials[1:4] <- "RS:"
lawson1t$initials[5:7] <- "EL:"
#remove initials from text
lawson1t$text <- str_replace_all(lawson1t$text, "[A-Z]{1,3}:", "")
#remove white spaces
lawson1t$text <- gsub("\\s{2,}", "", lawson1t$text)
#remove empty rows
lawson1t <- lawson1t[!is.na(lawson1t$text), ]
#merging text
lawson1t <- lawson1t %>%
  mutate(group = cumsum(initials != lag(initials, default = "")))
#merge consecutive rows within each group to have
lawson1t_merged <- lawson1t %>%
  group_by(initials, group) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup() %>% 
  arrange(group)
#extract time stamps
lawson1t_merged <- lawson1t_merged %>%
  mutate(time = str_extract_all(text, "\\[\\d{2}:\\d{2}:\\d{2}\\.\\d\\]"))
time_unnested <- lawson1t_merged %>%
  unnest(time)
#remove time stamps from text
lawson1t_merged$text <- gsub("\\[\\d{2}:\\d{2}:\\d{2}\\.\\d\\]", "",lawson1t_merged$text)
```

#### Create a function to extract a keyword and extract the beginning of the sentence, and end of the sentence containing that key word. This creates the lawson1t_merged_keyword containing the column "before", "keyword_occurences", "after" and "count_keyword". The issue with this current code is for the rows/groups where the keyword occurs more than one time in different sentences. This code will only extract the beginning and end of the sentence for one keyword per row --> this should be fixed in the future.

```{r extract_keyword}
# Choose a  keyword
new_keyword <- "traps"  # change keyword here

# Function to extract keyword and words before/after it
extract_keyword_and_context <- function(text, keyword) {
  keyword_indices <- gregexpr(paste0("(?i)\\b", keyword, "\\b"), text)
  num_occurrences <- sum(keyword_indices[[1]] != -1)
  
  if (num_occurrences == 0) {
    return(list(before = NULL, after = NULL, count = num_occurrences, keyword_occurrences = ""))
  }
  
  keyword_occurrences <- unlist(regmatches(text, keyword_indices))
  
   before <- sub(".*[.!?] ", "", substr(text, 1, min(keyword_indices[[1]] + attr(keyword_indices[[1]], "match.length")) - 1))
  before <- gsub(paste0("(?i)\\b", keyword, "\\b"), "", before)  
  after <- sub(paste0(".*(?i)\\b", keyword, "\\b(.*?[.!?])"), "\\1", text)
  
  return(list(keyword = keyword, before = before, after = after, count = num_occurrences, keyword_occurrences = keyword_occurrences))
}

# Function to process data with a given keyword
process_data_with_keyword <- function(data, keyword) {
  # Extract keyword and context for each row in the dataframe
  processed_data <- data %>%
    rowwise() %>%
    mutate(keyword_context = list(extract_keyword_and_context(text, keyword)))
  
  # Unnest the keyword_context column to get separate before and after columns
  processed_data <- processed_data %>%
    unnest_wider(c(keyword_context)) %>%
    mutate(
      before = coalesce(before, ""),
      after = coalesce(after, ""),
      count_keyword = coalesce(count, 0)
    ) %>% 
    select(initials, group, text, time, before, keyword_occurrences, after, count_keyword)
  
  return(processed_data)
}

# Process data with the new keyword
lawson1t_merged_keyword <- process_data_with_keyword(lawson1t_merged, new_keyword)

#Pull part of text to verify accuracy/see whole entry
lawson1t_merged_keyword %>% 
  filter(group == 54) %>%
  pull(text)
```

#### A list of location mentionned in Edwin Lawson's first interview was collected trough ChatGPT and is called locations. A list of all locations mentioned was created as Delphine and Will were coding the 4 EL interviews, it therefore contains more locations and is called locations_EL.

```{r location_list_chatgpt}
# List of locations from ChatGPT with EL interview 1 only
locations <- c("West Tremont", "home", "Seal Cove", "Ship Harbor", "Tremont",
               "Rockland", "Maine", "Swan's Island", "Bass Harbor", "Blue Hill Bay",
               "Hardwood Island", "Stonington", "Blue Hill", "Black Island", "Goose Cove", "England")
```

```{r location_list_EL_compiled}
# List of locations compiled by us while coding all the EL interviews
locations_EL <- c("Swan's Island", "home", "Southwest Harbor", "Manset", "Worcester", "Massachusetts", "California", "Pasadena", "Florida", "Lake Worth", "Palm Beach", "Westward", "Cherryfield", "Canada", "Hancock", "Bangor", "Castine", "Ship Harbor", "West", "Boston", "Gloucester", "New York", "Black Island", "Mitchell's Cove", "Placentia", "Bass Harbor", "Blue Hill", "Blue Hill Bay", "Hardwood Island", "Stonington", "Seal Cove", "Mitchellâ€™s Cove", "Goose Cove", "West Tremont", "Lynn", "Marblehead", "Maine", "Bass Harbor Head", "Bear Island", "Ship Island Ledges", "Ship Island", "bay", "Gulf", "Wilson", "Bernard", "Portland", "Rockland", "Mt. Desert Island/ Mount Desert Island", "Chester", "England", "Tremont", "Ellsworth", "Cranberry Island", "Mt. Desert Rock/ Mount Desert Rocks", "Duck Island", "Duck Cove", "Cape Cod", "Camden", "Norway", "Saugus", "Nova Scotia", "Atlantic", "Brooklin", "Corea", "Prospect Harbor", "Orono", "New Hampshire", "Hancock county", "Bar Harbor", "Northeast Harbor", "Vinalhaven", "Eastward", "Oak Point", "Long Ledge", "Platt's Point", "Somesville", "England")
```

#### A function is created to extract the locations from the the text column of lawson1t_merged in relations to the locations of the list location.The locations are extracted into a column called location.

```{r function_location_list}
#function to extract location
extract_location <- function(text) 
  { extracted_location <- str_extract_all(text, paste(locations, collapse = "|"))
  return(extracted_location)}
#link to lawson1t_merged
lawson1t_merged$location <- sapply(lawson1t_merged$text, extract_location)
#check how many picked out by chatGPT --> 21 rows with locations
lawson1t_merged %>% 
  count(location) %>% 
  summarize(sum = location)
```

#### This is a function extracting the locations from the text in reference to the locations_EL to make sure that ChatGPT did not miss any locations from the girst interview when it was create the location list. There are slight differences depending on considering West Tremont also as West and Tremont on their own. 

```{r function_location_EL_list}
# #function to extract locations from compiled EL list
extract_locations_EL <- function(text)
  { extracted_locations_EL <- str_extract_all(text, paste(locations_EL, collapse = "|"))
  return(extracted_locations_EL)}
# #link to lawson1t_merged
lawson1t_merged$location <- sapply(lawson1t_merged$text, extract_locations_EL)
#check how many picked out by chatGPT --> 21 rows with locations
lawson1t_merged %>%
  count(location) %>%
  summarize(sum = location)
#this gives 5 more locations than the list from ChatGPT. It separated West Tremont into two like West and Tremont. These have been mentioned both together and as separate. The locations were still picked out wich means that ChatGPT was still accurate. However by only using ChatGPT we might also miss a few.
```

#### This was a trial of Location network trough the lawson1t_merged dataset that was filtered for only the rows with locations extracted based on the location list. This network does not work. It was left here since it could potentially be modified and use the locations as the center bubbles and have relevant words/text hovering around. This visualization attempt could also be deleted.

```{r location_network_trial1, fig.height=15, fig.width=15}
#This one is not working (might want to use this template for locations and their keywords associated to it)
# Filter for the rows with locations
filtered_merged <- lawson1t_merged %>%
  filter(location != "character(0)") %>% 
  unnest(location)

# Add unique identifiers to location names
filtered_merged$location <- paste0(filtered_merged$location, "_", filtered_merged$group)

# Create list of connection locations (within the same group)
connection_list_1 <- filtered_merged %>%
  group_by(group) %>%
  filter(n() > 1) %>%
  mutate(from = lag(location), to = lead(location)) %>%
  na.omit() %>%
  select(from, to)

# Create a graph from the filtered data
network_location_1 <- graph_from_data_frame(connection_list_1, directed = TRUE)

# Get unique initials
initials <- unique(filtered_merged$initials)

# Choose colors
colors <- ifelse(grepl("^RS:", initials), "red", "blue")

# Plot network
plot(network_location_1, vertex.color = colors)
```

#### Here is a second try at a location network, and it also doesn't work. It was left here for someone to pick up or modify like mentioned for the first location network graph, but it could also be deleted. 

```{r network_location_trial2}
#This one is not working
# Filter for the rows with locations
filtered_merged <- lawson1t_merged %>%
  filter(location != "character(0)") %>% 
  unnest(location)

# Add unique identifiers to location names
#filtered_merged$location <- paste0(filtered_merged$location, "_", filtered_merged$group)

# Create list of connection locations (within the same group)
connection_list_2 <- filtered_merged %>%
  group_by(group, initials) %>%
  filter(n() > 1) #%>%
#  mutate(from = paste0(location, "_", initials), to = paste0(lead(location), "_", lead(initials))) %>%
  # na.omit() %>%
  # select(from, to)

# Create a graph from the filtered data
network_location_2 <- graph_from_data_frame(d = connection_list_2, directed = TRUE)

# # Get unique initials
# initials <- unique(filtered_merged$initials)

# Assign unique vertex names
# unique_names <- paste0(V(network_location)$name, "_", sequence(vcount(network_location)))
# V(network_location)$name <- unique_names

# Choose colors
colors <- ifelse(grepl("^RS:", initials), "red", "blue")

# Plot network with vertex color based on initials
plot(network_location_2, vertex.color = colors[V(network_location_2)$initials])
```

#### This is a thrid trial at a location netwrok graph, and IT WORKS! It is grouping all the locations by groups and creating a column from and a column to in the connection_list_3 in order to get all associations of locations that were mentioned within the same group.An attempt at calculating the connections for each node was made in order to display the size of the location according to its frequency/amount of connections. This part of the visualization is currently not working. It could be left as is, or modified to eventually work. The formatting and size of the colour legend is also not working superwell. The goal was to have it more on the vertical side. It could be modified or left as is. Overall this visualizations is valuable even if not perfect.

```{r geom_edge_link_location, fig.height=12, fig.width=30}
# Mostly working (good enough for now)
connection_list_3 <- connection_list_2 %>%
  group_by(group) %>%
  summarize(combination = list(combn(location, 2, simplify = FALSE))) %>% #create associations
  unnest(combination) %>%
  separate(combination, into = c("from", "to"), sep = ", ", convert = TRUE) %>% 
  mutate(from = gsub("^c\\(|\\)$|\"|'", "", from),
         to = gsub("^c\\(|\\)$|\"|'", "", to))

# Calculate the number of connections for each node (to display the size accordingly)
node_connections <- connection_list_3 %>%
  count(from, name = "connections") %>%
  bind_rows(connection_list_3 %>% count(to, name = "connections")) %>%
  group_by(from) %>%
  summarize(max_connections = max(connections, na.rm = TRUE))

# Merge node_degrees
connection_list_3 <- left_join(connection_list_3, node_connections, by = c("from" = "from"))

# Combine the 'group' and '(from, to)' variables to create unique ids
connection_list_3 <- connection_list_3 %>%
  mutate(unique_id = interaction(group, from, to))

connection_list_3 %>%
  ggraph(layout = "nicely") + #see other layout options
  geom_edge_link(aes(alpha = 2, 
                     color = unique_id),  #to color by group
                 show.legend = TRUE, #put to false to only have colors displayed but not legend
                 arrow = arrow(length = unit(1.5, "mm")),
                 start_cap = circle(3, "mm"),
                 end_cap = circle(3, "mm")) +
  geom_node_text(aes(label = name),
                 size = 6) +  # size by max_connection doesn't work here
  scale_size_continuous(range = c(6, 12), 
                        guide = "legend") +  # legend of size not working
  scale_color_viridis_d() +
  # theme(legend.direction = "vertical", 
  #       legend.text = element_text(size = 12), 
  #       legend.key.width = unit(2, "cm")) + #this formatting of the color legend is also not working
  theme_graph() 
```

#### This code aims to generalize the steps used above for the location netwrok visualizations so that it could be included in the MOS Shiny App as a tab. It is all commented in order to knit the document since it it referencing datasets that are not present in this Rmd file, since it is meant to work with the workflow set up in the App.

```{r general__steps_location_function}
# location_connections <- cleaned_interview %>%
#   filter(location != "character(0)") %>% # Filter for the rows with locations only
#   unnest(location) %>% 
#   group_by(group, initials) %>%
#   filter(n() > 1)  #filter rows with more than one location
#   group_by(group) %>%
#     summarize(combination = list(combn(location, 2, simplify = FALSE))) %>% #create associations
#     unnest(combination) %>%
#     separate(combination, into = c("from", "to"), sep = ", ", convert = TRUE) %>% 
#     mutate(from = gsub("^c\\(|\\)$|\"|'", "", from),
#            to = gsub("^c\\(|\\)$|\"|'", "", to))
#   
# # Calculate the number of connections for each node (to display the size accordingly)
# node_connections <- location_connections %>%
#   count(from, name = "connections") %>%
#   bind_rows(location_connections %>% count(to, name = "connections")) %>%
#   group_by(from) %>%
#   summarize(max_connections = max(connections, na.rm = TRUE))
# 
# # Merge node_degrees
# location_connections <- left_join(location_connections, node_connections, by = c("from" = "from"))
#   
# # Combine the 'group' and '(from, to)' variables to create unique ids
# location_connections_3 <- location_connections %>%
#   mutate(unique_id = interaction(group, from, to))
# 
# location_connections %>%
#   ggraph(layout = "nicely") + #see other layout options
#   geom_edge_link(aes(alpha = 2, 
#                      color = unique_id),  #to color by group
#                  show.legend = TRUE, #put to false to only have colors displayed but not legend
#                  arrow = arrow(length = unit(1.5, "mm")),
#                  start_cap = circle(3, "mm"),
#                  end_cap = circle(3, "mm")) +
#   geom_node_text(aes(label = name),
#                  size = 6) +  # size by max_connection doesn't work here
#   scale_size_continuous(range = c(6, 12), 
#                         guide = "legend") +  # legend of size not working
#   scale_color_viridis_d() +
#   theme_graph() 
```

#### This chunck uses the more generic code above to create the functions that could be included in the Shiny App. This is a "best guess" but it has not yet been tried in the app and it might need some modifications before it actually. It is all commented out for the same reasons as the chunk above. Be careful because of the line in this chunk (and the one above) are actual comments to express the different steps of the code. 
```{r location_function}
# create_location_connections <- function(cleaned_interview) {
#   # Filter for the rows with locations only
#   location_connections <- cleaned_interview %>%
#     filter(!is.na(extracted_locations)) %>%
#     unnest_tokens(extracted_locations) %>%
#     group_by(group, initials) %>%
#     filter(n() > 1)  # Filter rows with more than one location
# 
#   # Create associations between locations
#   location_connections <- location_connections %>%
#     group_by(group) %>%
#     summarize(combination = list(combn(extracted_locations, 2, simplify = FALSE))) %>%
#     unnest(combination) %>%
#     separate(combination, into = c("from", "to"), sep = ", ", convert = TRUE) %>%
#     mutate(from = gsub("^c\\(|\\)$|\"|'", "", from),
#            to = gsub("^c\\(|\\)$|\"|'", "", to))
# 
#   # Calculate the number of connections for each node (to display the size accordingly)
#   node_connections <- location_connections %>%
#     count(from, name = "connections") %>%
#     bind_rows(location_connections %>% count(to, name = "connections")) %>%
#     group_by(from) %>%
#     summarize(max_connections = max(connections, na.rm = TRUE))
# 
#   # Merge node_degrees
#   location_connections <- left_join(location_connections, node_connections, by = c("from" = "from"))
# 
#   # Combine the 'group' and '(from, to)' variables to create unique ids
#   location_connections$unique_id <- interaction(location_connections$group, location_connections$from, location_connections$to)
# 
#   return(location_connections)
# }
# 
#   # Create the location connections dataframe using the cleaned_interview
#   location_connections <- reactive({
#     req(cleaned_interview())
#     create_location_connections(cleaned_interview())
#   })
#   
#   # Generate the location network visualization
#   output$location_network <- renderPlot({
#     location_connections_data <- location_connections()
#     
#     if (nrow(location_connections_data) > 0) {
#       location_connections_data %>%
#         ggraph(layout = "nicely") +
#         geom_edge_link(aes(alpha = 2, color = unique_id),
#                        show.legend = TRUE,
#                        arrow = arrow(length = unit(1.5, "mm")),
#                        start_cap = circle(3, "mm"),
#                        end_cap = circle(3, "mm")) +
#         geom_node_text(aes(label = from),
#                        size = 6) +
#         scale_size_continuous(range = c(6, 12), guide = "legend") +
#         scale_color_viridis_d() +
#         theme_graph()
#     } else {
#       # If there are no location connections, display a message
#       plot(1, 1, type = "n", xlab = "", ylab = "", xlim = c(0, 1), ylim = c(0, 1))
#       text(0.5, 0.5, "No location connections found.", cex = 1.2)
#     }
#   })
# }
```

```{r location_testing}
# location_connections <- create_location_connections("connection_list_2")
```

#### This is a list of gear that was extracted by ChatGPT from Edwin Lawson's first interview. This list is also what is currently in the Shiny App. It is far from being comprehensive and some words might not be appropriate, but we left it as it since we were using it for trials and creating the App rather than actually using it to code for interviews. It will need to be changed and made more comprehensive in the future.

```{r gear_list}
gear <- c("tape recorder", "battery", "sardine weir", "fishing lines", "lobster traps", "rowboat", "lobster smack", "lobster buyers", "lobster companies", "lobster boat", "lobster catcher", "outboard motor", "two and a half horsepower outboard motor", "wooden lobster traps", "softwood traps", "second-hand traps", "laths", "boughs", "bottoms", "frames", "nylon twine", "cotton twine", "manila twine", "polyethylene twine", "plastic buoys", "foam buoys", "styrofoam buoys", "wooden buoys", "cedar wood", "oak wood", "spruce wood", "pot warp", "toggle", "single-cylinder boat engine", "four-cylinder boat engine", "six-cylinder boat engine", "eight-cylinder boat engine", "diesel engine", "gasoline engine", "hundred and thirty-five horsepower gasoline engine", "eighty-horsepower diesel engine","seine net")
```

#### Here we create the lawson1t_merged_lower to more easily extract all the grea from the text column. This was not done earlier since we needed the Uppercase words to search and match the locations.
```{r lower_case_text}
lawson1t_merged_lower <- lawson1t_merged %>%
  mutate(text = tolower(text))
```

#### This is a function to extract the gear from the text column and put it in a gear column (similar to locations)
```{r function_gear_list}
#function to extract gear
extract_gear <- function(text) 
  { extracted_gear <- str_extract_all(text, paste(gear, collapse = "|"))
  return(extracted_gear)}
#link to lawson1t_merged_lower
lawson1t_merged_lower$gear <- sapply(lawson1t_merged_lower$text, extract_gear)
```

#### This is a list of species extracted by ChatGPT from the first EL interview. Like for the gear and locations, it will need to be enhanced.

```{r species_list}
species <- c("lobsters", "scallops", "lobster", "scallop", "sardine", "sardines")
```

#### This is the function that extracts the species from the text in reference to the species list and put them in a column called species, using the lawson1t_merged_lower dataset. 
```{r species_list_function}
#function to extract species
extract_species <- function(text) 
  { extracted_species <- str_extract_all(text, paste(species, collapse = "|"))
  return(extracted_species)}
#link to lawson1t_merged_lower
lawson1t_merged_lower$species <- sapply(lawson1t_merged_lower$text, extract_species)
```

#### THis was a trial at text wrapping in RStudio for the table in order to better see the text column. It is not working here, but ChatGPT says that it will be more useful within the shiny App --> as a matter of fact, using data.table in the Shiny App is helpful and allows for a more user friendly data table that the user can also modify.
```{r text-wrapping}
# Convert to data.table
setDT(lawson1t_merged_lower)

# Function to wrap the text and create a new data.table with individual wrapped lines
wrap_text_lines <- function(text, width) {
  wrapped_lines <- strwrap(text, width = width)
  data.table(Wrappedtext = unlist(wrapped_lines))
}

# Wrap the text in the "text" column and store the result in the new data.table
width <- 1000  # Adjust the width as needed
wrap_lawson1t_merged_lower <- lawson1t_merged_lower[, wrap_text_lines(text, width), by = group]

# Print the modified data.table with wrapped text
print(wrap_lawson1t_merged_lower)
```

#### This is a generic term frequency graph for the first Edwin Lawson interview, created trough the lawson1_word dataset where the text column was unnested to separate each word.

```{r word_frequency_plot}
lawson1_word <- lawson1t %>% 
  unnest_tokens(word, text) 
lawson1_word %>% 
  anti_join(get_stopwords(source = "smart")) %>% 
  count(word, sort = TRUE) %>%
  slice_max(n, n = 20) %>%
  ggplot(aes(n, fct_reorder(word, n))) + 
  geom_col()
```

#### This is a calculation of the term frequency values for EL1

```{r term_frequency}
lawson1_word <- lawson1t %>%
  unnest_tokens(word, text) 

lawson1_word <- lawson1_word %>%
  anti_join(get_stopwords(source = "smart")) %>%
  count(word, sort = TRUE)

word_document_freq_1 <- lawson1_word %>%
  mutate(tf = n / sum(n)) 

print(word_document_freq_1)
```

#### The word_list was created by asking ChapGPT for the words with the higest frequency and that were the most relevant to the content of the first Edwin Lawson Interview. We beleived it could give a more meaningful summary of the content of the interview than the top words from the word frequency. A wordcloud was then created using this word_list.

```{r list_frequencies_wordcloud, fig.height=8, fig.width=8}
#words provided by ChatGPT + added bay, summer, harbor, dollars from the word frequency plot above. (total of 37 words --> decide what amount of words would be best) I also added weiring and dragging (they are not of high frequency, but it is mentioned, which would be good to portray)
word_list <- c("lobster", "boat", "traps", "fishermen", "water", "lobstering", "oil", "clothes",
               "motor", "work", "years", "fishing", "rowboat", "wharf", "season", "boats",
               "diesel", "buoys", "accidents", "licence", "gasoline", "engine", "swim", "son",
               "horsepower", "fishing", "heavy", "boots", "color", "land", "coast", "shore",
               "family", "winter", "father", "summer", "bay", "harbor", "dollars", "weiring", "dragging")
words_freq_1 <- lawson1_word
words_freq_1 <- words_freq_1[words_freq_1$word %in% word_list, ]
color_palette <- brewer.pal(8, "Set2")
wordcloud(
  words_freq_1$word,  
  words_freq_1$n,
  colors = color_palette,
  random.order = FALSE,  # To keep the word order as per frequency
  scale = c(5, 0.5),      
  max.words = 50,
)
```

#### This was an attempt at topic modeling, which was not super conclusive since it was performed only with the first Edwin Lawson Interview. This will need to be done with a corpus of interview to become relevant!

```{r LDA_topic_modeling}
lawson1_word <- lawson1_word  %>% 
  anti_join(get_stopwords(source = "smart")) %>% 
  mutate(document_id = row_number())
#create document term matrix (DTM)
dtm <- lawson1_word %>%
  count(document_id, word) %>%
  cast_dtm(document_id, word, n)
#set number of topics
num_topics <- 20
#run Latent Dirichlet Allocation (LDA) topic modeling algorithm 
lda_model <- LDA(dtm, k = num_topics)
#most probable words for each topic
terms_per_topic <- terms(lda_model, 10)
#get most dominant topic for each document
document_topics <- tidy(lda_model, matrix = "gamma") %>%
  group_by(document) %>%
  top_n(1, gamma)
```

#### Reading in the second Edwin Lawson interview (10201972) and creating the lawson2_full_text dataset (like for EL1)
```{r read_data_EL2}
lawson2_full_text <- pdftools::pdf_text(pdf = "data/Lawson_Edwin_10201972.pdf")
```

#### Creating the lawson_2 tibble of text (like for EL1)
```{r string-data_EL2}
# convert text to string
lawson2_full_text <- toString(lawson2_full_text)
lawson2_full_text
# convert text to character lines
lawson2 <- read_lines(lawson2_full_text)

lawson2 <- tibble(lawson2)
lawson2 <- rename(lawson2, text = lawson2)
```

#### Cleaning table for EL2, like done previously for EL1
```{r data_cleaning_EL2}
lawson2 <- lawson2 %>% 
  mutate(initials = str_extract(text, pattern = "[A-Z]{1,3}\\:")) %>%
  fill(initials, .direction = "down")
#remove the metadata and only look at interview
lawson2t <- lawson2[-c(1:57), ]
#correct first initials
 lawson2t$initials[1:9] <- "ML:"
 lawson2t$initials[10:11] <- "RS:"
 lawson2t$initials[12:13] <- "EL:"
#remove full names
 lawson2t$text[10] <- gsub("Rita Swidrowski:", "",  lawson2t$text[10])
 lawson2t$text[12] <- gsub("Edwin Lawson:", "",  lawson2t$text[12])
#remove initials from text
lawson2t$text <- str_replace_all(lawson2t$text, "[A-Z]{1,3}:", "")
#remove white spaces
lawson2t$text <- gsub("\\s{2,}", "", lawson2t$text)
#remove empty rows
lawson2t <- lawson2t[!is.na(lawson2t$text), ]
#merging text
lawson2t <- lawson2t %>%
  mutate(group = cumsum(initials != lag(initials, default = "")))
#merge consecutive rows within each group to have
lawson2t_merged <- lawson2t %>%
  group_by(initials, group) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  ungroup() %>% 
  arrange(group)
#extract time stamps
lawson2t_merged <- lawson2t_merged %>%
  mutate(time = str_extract_all(text, "\\[\\d{2}:\\d{2}:\\d{2}\\.\\d{2}\\]"))
time_unnested <- lawson2t_merged %>%
  unnest(time)
#remove time stamps from text
lawson2t_merged$text <- gsub("\\[\\d{2}:\\d{2}:\\d{2}\\.\\d{2}\\]", "",lawson2t_merged$text)
```

#### Term Frequency calculation for the EL2
```{r term_frequency_EL2}
lawson2_word <- lawson2t %>%
  unnest_tokens(word, text) 

lawson2_word <- lawson2_word %>%
  anti_join(get_stopwords(source = "smart")) %>% 
  count(word, sort = TRUE)

word_document_freq_2 <- lawson2_word %>%
  mutate(tf = n / sum(n)) 

print(word_document_freq_2)
```

#### TF-IDF between EL1 and EL2. Here we seen that more stop words need to be excluded to get more relevant results.

```{r tf-idf_EL1_EL2}
#do we perhaps also want to create a running list of our own stop_words to add so that we can remove them and they won't show up here (eg: that's, there's, they're, can't)
lawson1t <- lawson1t %>% 
  mutate(interview = "EL1")

lawson2t <- lawson2t %>% 
  mutate(interview = "EL2")

EL_interviews <- bind_rows(lawson1t, lawson2t)

interviews_word <- EL_interviews %>%
  unnest_tokens(word, text) %>% 
  anti_join(get_stopwords(source = "smart")) %>% 
  count(interview, word, sort = TRUE)

interviews_tf_idf <- interviews_word %>%
  bind_tf_idf(word, interview, n)
interviews_tf_idf %>%
  arrange(-tf_idf)
```
#### Word Frequency Plot TF-IDF (EL1 and EL2)

```{r tf-idf_word_frequency_viz, fig.height=8, fig.width=5}
interviews_tf_idf %>%
  group_by(interview) %>%
  slice_max(tf_idf, n = 20) %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = interview)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~interview, scales = "free")
```

#### WordCloud visualization based on TF-IDF (EL1 and EL2)

```{r word_cloud_tf-idf, fig.height=8, fig.width=8}
wordcloud(
  words = interviews_tf_idf$word,
  freq = interviews_tf_idf$tf_idf,
  scale = c(5, 0.5),
  colors = brewer.pal(length(unique(interviews_tf_idf$interview)), "Set2"),
  max.words = 40,
  random.order = FALSE
)
legend("topright", legend = unique(interviews_tf_idf$interview), fill = colors)
#Warning: minimal value for n is 3, returning requested palette with 3 different levels
```

